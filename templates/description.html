<h1>Shared Task on Automatic Evaluation for Code-Switched Text Generation</h1>

<h2>Task Description</h2>
<p>This shared task focuses on developing automatic evaluation metrics for code-switched (CS) text generation. Participants are tasked with creating systems that can accurately assess the quality of synthetically generated CS text, considering both fluency and accuracy.</p>

<h3>Why is this important?</h3>
<ul>
    <li><strong>Scarcity of CS Data:</strong> CS text data is limited, making automatic generation vital for data augmentation and improving model performance.</li>
    <li><strong>Growing Demand:</strong> The need for CS text is increasing, particularly in dialogue systems and chatbots, to enable more natural and inclusive interactions.</li>
    <li><strong>Lack of Robust Evaluation:</strong> Current methods for evaluating CS text are insufficient, hindering progress in this field.</li>
</ul>

<h2>Languages Supported</h2>
<h3>Public Leaderboard:</h3>
<ul>
    <li>English-Hindi</li>
    <li>English-Tamil</li>
    <li>English-Malayalam</li>
</ul>

<h3>Private Leaderboard:</h3>
<ul>
    <li>English-Indonesian</li>
    <li>Indonesian-Javanese</li>
    <li>Singlish (English-Chinese)</li>
</ul>
